{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9496454,"sourceType":"datasetVersion","datasetId":5778756}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T17:01:42.556975Z","iopub.execute_input":"2025-02-14T17:01:42.557437Z","iopub.status.idle":"2025-02-14T17:01:43.083629Z","shell.execute_reply.started":"2025-02-14T17:01:42.557391Z","shell.execute_reply":"2025-02-14T17:01:43.082331Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/debugging-a-sales-data-workflow/sales.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\ndef load_and_check():\n    # Step 1: Load the data and check if it has the expected shape\n    data = pd.read_csv('/kaggle/input/debugging-a-sales-data-workflow/sales.csv')  \n    \n    expected_columns = 17\n    actual_columns = data.shape[1]\n    if actual_columns != expected_columns:\n        print(f\"Data column mismatch! Expected {expected_columns}, but got {actual_columns}.\")\n        print(f\"Columns found: {list(data.columns)}\")\n    else:\n        print(\"Data loaded successfully.\")\n\n    # Step 2: Calculate statistical values and merge with the original data\n    grouped_data = data.groupby(['Date'])['Total'].agg(['mean', 'std'])\n    grouped_data['threshold'] = 3 * grouped_data['std']\n    grouped_data['max'] = grouped_data['mean'] + grouped_data.threshold\n    grouped_data['min'] = grouped_data[['mean', 'threshold']].apply(lambda row: max(0, row['mean'] - row['threshold']), axis=1)\n    data = pd.merge(data, grouped_data, on='Date', how='left')\n\n    # Condition_1 checks if 'Total' is within the acceptable range (min to max) for each date\n    data['Condition_1'] = (data['Total'] >= data['min']) & (data['Total'] <= data['max'])\n    data['Condition_1'].fillna(False, inplace=True)  \n\n\n    # Condition_2 checks if the 'Tax' column is properly calculated as 5% of (Quantity * Unit price)\n    data[('Tax')] = data['Quantity'] * data['Unit price'] * 0.05\n    data['Condition_2'] = round(data['Quantity'] * data['Unit price'] * 0.05, 1) == round(data['Tax'], 1)\n        \n\n    # Step 3: Check if all rows pass both Condition_1 and Condition_2\n    # Success indicates data integrity; failure suggests potential issues.\n    failed_condition_1 = data[~data['Condition_1']]\n    failed_condition_2 = data[~data['Condition_2']]\n\n    if failed_condition_1.shape[0] > 0 or failed_condition_2.shape[0] > 0:\n        print(f\"Data integrity check failed! {failed_condition_1.shape[0]} rows failed Condition_1, \"\n              f\"{failed_condition_2.shape[0]} rows failed Condition_2.\")\n    else:\n        print(\"Data integrity check was successful! All rows pass the integrity conditions.\")\n        \n    return data\n\nprocessed_data = load_and_check()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T17:02:12.930036Z","iopub.execute_input":"2025-02-14T17:02:12.930421Z","iopub.status.idle":"2025-02-14T17:02:12.969587Z","shell.execute_reply.started":"2025-02-14T17:02:12.930392Z","shell.execute_reply":"2025-02-14T17:02:12.967366Z"}},"outputs":[{"name":"stdout","text":"Data loaded successfully.\nData integrity check was successful! All rows pass the integrity conditions.\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-5-43c632faab06>:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['Condition_1'].fillna(False, inplace=True)\n","output_type":"stream"}],"execution_count":5}]}